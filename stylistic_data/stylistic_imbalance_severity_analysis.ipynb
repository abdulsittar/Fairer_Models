{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stylistic Synthetic Data: Imbalance Severity Analysis\n",
    "\n",
    "This notebook conducts a **comprehensive imbalance severity analysis** using our new **stylistic synthetic tweet data** to test whether the improved generation approach changes effectiveness under different levels of class imbalance.\n",
    "\n",
    "## üéØ Research Hypothesis\n",
    "\n",
    "Our stylistic synthetic data (generated with proper fake tweet linguistic patterns) may show **increasing advantages over traditional methods under severe class imbalance**, where more realistic synthetic examples become crucial for learning minority class boundaries.\n",
    "\n",
    "## üÜï Key Improvements Over Previous Analysis\n",
    "\n",
    "1. **Better Synthetic Data**: Uses stylistic synthetic tweets that match fake tweet vocabulary, exclamation patterns, and topics\n",
    "2. **Optimal Model**: Random Forest with Count Vectorization (best from previous experiments: 94.39% F1)\n",
    "3. **Real Baseline**: Generated synthetic data cost $0.33 vs fact-manipulation approach\n",
    "4. **Targeted Features**: Synthetic tweets designed to match the 10 most distinguishing fake tweet features\n",
    "\n",
    "## üî¨ Experimental Design\n",
    "\n",
    "We test **4 different imbalance levels** by systematically reducing dataset size while maintaining the same absolute class gap (3,772):\n",
    "\n",
    "1. **2.8% Imbalance**: ~134K total samples, 48.6% minority ‚Üí **Baseline (nearly balanced)**\n",
    "2. **5.6% Imbalance**: ~67K total samples, ~44% minority ‚Üí **Mild imbalance** \n",
    "3. **9.4% Imbalance**: ~40K total samples, ~40% minority ‚Üí **Moderate imbalance**\n",
    "4. **28.4% Imbalance**: ~13K total samples, ~22% minority ‚Üí **Severe imbalance**\n",
    "\n",
    "## üìä Testing Framework\n",
    "\n",
    "For each imbalance level, we test **6 sampling strategies**:\n",
    "1. **Original unbalanced** (baseline)\n",
    "2. **Undersampled majority** (reduce real tweets)\n",
    "3. **Traditional oversampling** (duplicate random fake tweets)\n",
    "4. **10% Stylistic synthetic** (377 stylistic tweets)\n",
    "5. **50% Stylistic synthetic** (1,886 stylistic tweets)\n",
    "6. **100% Stylistic synthetic** (3,772 stylistic tweets)\n",
    "\n",
    "**Total experiments**: 4 imbalance levels √ó 6 strategies = **24 experiments**\n",
    "\n",
    "## üîç Research Questions\n",
    "\n",
    "1. **Does stylistic synthetic data outperform traditional methods under severe imbalance?**\n",
    "2. **How does the vocabulary/pattern matching approach scale with imbalance severity?**  \n",
    "3. **Are there threshold effects where linguistic realism becomes critical?**\n",
    "4. **Which sampling strategy is most robust across different imbalance levels?**\n",
    "5. **Does our $0.33 generation cost justify performance improvements?**\n",
    "\n",
    "## üé® Stylistic Synthetic Data Features\n",
    "\n",
    "Our synthetic tweets are designed to match fake tweet patterns:\n",
    "- **Vocabulary**: Biden, vaccine, fraud, election terms (6-24√ó more frequent in fake tweets)\n",
    "- **Style**: More exclamations (+56%), fewer hashtags (-36%), longer text (+6%)\n",
    "- **Topics**: Election fraud, COVID conspiracies, Biden criticism, government overreach\n",
    "- **Generation**: GPT-3.5 with carefully crafted prompts targeting linguistic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Libraries imported successfully\n",
      "üìÖ Analysis started at: 2025-08-18 18:24:33\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully\")\n",
    "print(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Imbalance Levels (constant 3,772 gap):\n",
      "   2.8%: 134,198 tweets (48.6% minority, 3,772 gap) - Baseline (nearly balanced) - 134K total\n",
      "   5.6%: 66,986 tweets (47.2% minority, 3,772 gap) - Mild imbalance - 67K total\n",
      "   9.4%: 40,000 tweets (45.3% minority, 3,772 gap) - Moderate imbalance - 40K total\n",
      "   25.1%: 15,000 tweets (37.4% minority, 3,772 gap) - Severe imbalance - 15K total\n",
      "\n",
      "üîß Sampling Strategies: 6\n",
      "   unbalanced: Original unbalanced\n",
      "   undersampling: Undersampled majority\n",
      "   random_oversampling: Traditional oversampling\n",
      "   stylistic_10: 10% Stylistic synthetic\n",
      "   stylistic_50: 50% Stylistic synthetic\n",
      "   stylistic_100: 100% Stylistic synthetic\n",
      "\n",
      "üßÆ Total experiments: 4 √ó 6 = 24\n",
      "\n",
      "‚úÖ Verification - All gaps should be 3,772:\n",
      "   2.8%: 3,772 gap (‚úÖ)\n",
      "   5.6%: 3,772 gap (‚úÖ)\n",
      "   9.4%: 3,772 gap (‚úÖ)\n",
      "   25.1%: 3,772 gap (‚úÖ)\n"
     ]
    }
   ],
   "source": [
    "# Define imbalance levels and sampling strategies\n",
    "\n",
    "# Imbalance levels (maintaining constant 3,772 gap between classes)\n",
    "# The key is that imbalance SEVERITY increases as total dataset size decreases\n",
    "IMBALANCE_LEVELS = {\n",
    "    \"2.8%\": {\"real\": 68985, \"fake\": 65213, \"description\": \"Baseline (nearly balanced) - 134K total\"},\n",
    "    \"5.6%\": {\"real\": 35379, \"fake\": 31607, \"description\": \"Mild imbalance - 67K total\"}, \n",
    "    \"9.4%\": {\"real\": 21886, \"fake\": 18114, \"description\": \"Moderate imbalance - 40K total\"},\n",
    "    \"25.1%\": {\"real\": 9386, \"fake\": 5614, \"description\": \"Severe imbalance - 15K total\"}\n",
    "}\n",
    "\n",
    "# Sampling strategies\n",
    "SAMPLING_STRATEGIES = {\n",
    "    \"unbalanced\": \"Original unbalanced\",\n",
    "    \"undersampling\": \"Undersampled majority\",\n",
    "    \"random_oversampling\": \"Traditional oversampling\", \n",
    "    \"stylistic_10\": \"10% Stylistic synthetic\",\n",
    "    \"stylistic_50\": \"50% Stylistic synthetic\",\n",
    "    \"stylistic_100\": \"100% Stylistic synthetic\"\n",
    "}\n",
    "\n",
    "print(\"üìä Imbalance Levels (constant 3,772 gap):\")\n",
    "for level, config in IMBALANCE_LEVELS.items():\n",
    "    minority_pct = config['fake'] / (config['real'] + config['fake']) * 100\n",
    "    total = config['real'] + config['fake']\n",
    "    gap = config['real'] - config['fake']\n",
    "    print(f\"   {level}: {total:,} tweets ({minority_pct:.1f}% minority, {gap:,} gap) - {config['description']}\")\n",
    "\n",
    "print(f\"\\nüîß Sampling Strategies: {len(SAMPLING_STRATEGIES)}\")\n",
    "for strategy, description in SAMPLING_STRATEGIES.items():\n",
    "    print(f\"   {strategy}: {description}\")\n",
    "    \n",
    "print(f\"\\nüßÆ Total experiments: {len(IMBALANCE_LEVELS)} √ó {len(SAMPLING_STRATEGIES)} = {len(IMBALANCE_LEVELS) * len(SAMPLING_STRATEGIES)}\")\n",
    "\n",
    "# Verify the constant gap\n",
    "print(f\"\\n‚úÖ Verification - All gaps should be 3,772:\")\n",
    "for level, config in IMBALANCE_LEVELS.items():\n",
    "    gap = config['real'] - config['fake'] \n",
    "    print(f\"   {level}: {gap:,} gap ({'‚úÖ' if gap == 3772 else '‚ùå'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Experiment function ready (Random Forest + Count Vectorization)\n"
     ]
    }
   ],
   "source": [
    "# Classification experiment function\n",
    "\n",
    "def run_imbalance_experiment(real_data, fake_data, experiment_name, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Run classification experiment with Random Forest + Count Vectorization\n",
    "    \n",
    "    Args:\n",
    "        real_data: List of real tweet texts\n",
    "        fake_data: List of fake tweet texts  \n",
    "        experiment_name: Name for this experiment\n",
    "        test_size: Proportion for test set\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    texts = real_data + fake_data\n",
    "    labels = [0] * len(real_data) + [1] * len(fake_data)  # 0=real, 1=fake\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Count Vectorization (best from previous experiments)\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Random Forest (best from previous experiments)\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fake_f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    overall_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Class distribution\n",
    "    real_count = len(real_data)\n",
    "    fake_count = len(fake_data) \n",
    "    minority_pct = fake_count / (real_count + fake_count) * 100\n",
    "    imbalance_ratio = real_count / fake_count\n",
    "    \n",
    "    return {\n",
    "        'experiment_name': experiment_name,\n",
    "        'fake_f1': fake_f1,\n",
    "        'overall_f1': overall_f1,\n",
    "        'real_count': real_count,\n",
    "        'fake_count': fake_count,\n",
    "        'total_count': real_count + fake_count,\n",
    "        'minority_percentage': minority_pct,\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Experiment function ready (Random Forest + Count Vectorization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset creation functions ready\n"
     ]
    }
   ],
   "source": [
    "# Create datasets for different imbalance levels\n",
    "\n",
    "def create_imbalanced_datasets(real_tweets, fake_tweets, imbalance_config):\n",
    "    \"\"\"\n",
    "    Create datasets with specified imbalance levels\n",
    "    \n",
    "    Args:\n",
    "        real_tweets: Full list of real tweets\n",
    "        fake_tweets: Full list of fake tweets\n",
    "        imbalance_config: Dictionary with 'real' and 'fake' counts\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (sampled_real, sampled_fake)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample down to required sizes\n",
    "    sampled_real = resample(\n",
    "        real_tweets,\n",
    "        n_samples=min(imbalance_config['real'], len(real_tweets)),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    sampled_fake = resample(\n",
    "        fake_tweets,\n",
    "        n_samples=min(imbalance_config['fake'], len(fake_tweets)),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return list(sampled_real), list(sampled_fake)\n",
    "\n",
    "\n",
    "def apply_sampling_strategy(real_data, fake_data, strategy, synthetic_tweets):\n",
    "    \"\"\"\n",
    "    Apply specific sampling strategy to balance datasets\n",
    "    \n",
    "    Args:\n",
    "        real_data: Real tweets for this imbalance level\n",
    "        fake_data: Fake tweets for this imbalance level\n",
    "        strategy: Sampling strategy name\n",
    "        synthetic_tweets: Stylistic synthetic tweets\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (processed_real, processed_fake)\n",
    "    \"\"\"\n",
    "    \n",
    "    if strategy == \"unbalanced\":\n",
    "        return real_data, fake_data\n",
    "        \n",
    "    elif strategy == \"undersampling\":\n",
    "        # Reduce real tweets to match fake count\n",
    "        undersampled_real = resample(real_data, n_samples=len(fake_data), random_state=42)\n",
    "        return list(undersampled_real), fake_data\n",
    "        \n",
    "    elif strategy == \"random_oversampling\":\n",
    "        # Duplicate random fake tweets to match real count\n",
    "        imbalance = len(real_data) - len(fake_data)\n",
    "        if imbalance > 0:\n",
    "            random_duplicates = resample(fake_data, n_samples=imbalance, random_state=42)\n",
    "            balanced_fake = fake_data + list(random_duplicates)\n",
    "            return real_data, balanced_fake\n",
    "        return real_data, fake_data\n",
    "        \n",
    "    elif strategy == \"stylistic_10\":\n",
    "        # Add 10% of synthetic tweets (377 tweets)\n",
    "        synthetic_sample = resample(synthetic_tweets, n_samples=377, random_state=42)\n",
    "        enhanced_fake = fake_data + list(synthetic_sample)\n",
    "        return real_data, enhanced_fake\n",
    "        \n",
    "    elif strategy == \"stylistic_50\":\n",
    "        # Add 50% of synthetic tweets (1,886 tweets)\n",
    "        synthetic_sample = resample(synthetic_tweets, n_samples=1886, random_state=42)\n",
    "        enhanced_fake = fake_data + list(synthetic_sample)\n",
    "        return real_data, enhanced_fake\n",
    "        \n",
    "    elif strategy == \"stylistic_100\":\n",
    "        # Add 100% of synthetic tweets (3,772 tweets)\n",
    "        enhanced_fake = fake_data + synthetic_tweets\n",
    "        return real_data, enhanced_fake\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "print(\"‚úÖ Dataset creation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Comprehensive Imbalance Severity Analysis\n",
      "======================================================================\n",
      "\n",
      "üìä IMBALANCE LEVEL: 2.8% (Baseline (nearly balanced) - 134K total)\n",
      "------------------------------------------------------------\n",
      "   üìè Dataset: 68,985 real, 65,213 fake (48.6% minority)\n",
      "\n",
      "   üß™ [ 1/24] Original unbalanced\n",
      "      ‚úÖ Fake F1: 0.9682 | Overall F1: 0.9691\n",
      "         Dataset: 68,985 real + 65,213 fake\n",
      "\n",
      "   üß™ [ 2/24] Undersampled majority\n",
      "      ‚úÖ Fake F1: 0.9733 | Overall F1: 0.9734\n",
      "         Dataset: 65,213 real + 65,213 fake\n",
      "\n",
      "   üß™ [ 3/24] Traditional oversampling\n",
      "      ‚úÖ Fake F1: 0.9714 | Overall F1: 0.9713\n",
      "         Dataset: 68,985 real + 68,985 fake\n",
      "\n",
      "   üß™ [ 4/24] 10% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9685 | Overall F1: 0.9693\n",
      "         Dataset: 68,985 real + 65,590 fake\n",
      "\n",
      "   üß™ [ 5/24] 50% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9696 | Overall F1: 0.9701\n",
      "         Dataset: 68,985 real + 67,099 fake\n",
      "\n",
      "   üß™ [ 6/24] 100% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9708 | Overall F1: 0.9708\n",
      "         Dataset: 68,985 real + 68,985 fake\n",
      "\n",
      "üìä IMBALANCE LEVEL: 5.6% (Mild imbalance - 67K total)\n",
      "------------------------------------------------------------\n",
      "   üìè Dataset: 35,379 real, 31,607 fake (47.2% minority)\n",
      "\n",
      "   üß™ [ 7/24] Original unbalanced\n",
      "      ‚úÖ Fake F1: 0.9532 | Overall F1: 0.9556\n",
      "         Dataset: 35,379 real + 31,607 fake\n",
      "\n",
      "   üß™ [ 8/24] Undersampled majority\n",
      "      ‚úÖ Fake F1: 0.9603 | Overall F1: 0.9605\n",
      "         Dataset: 31,607 real + 31,607 fake\n",
      "\n",
      "   üß™ [ 9/24] Traditional oversampling\n",
      "      ‚úÖ Fake F1: 0.9594 | Overall F1: 0.9592\n",
      "         Dataset: 35,379 real + 35,379 fake\n",
      "\n",
      "   üß™ [10/24] 10% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9544 | Overall F1: 0.9566\n",
      "         Dataset: 35,379 real + 31,984 fake\n",
      "\n",
      "   üß™ [11/24] 50% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9543 | Overall F1: 0.9554\n",
      "         Dataset: 35,379 real + 33,493 fake\n",
      "\n",
      "   üß™ [12/24] 100% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9570 | Overall F1: 0.9568\n",
      "         Dataset: 35,379 real + 35,379 fake\n",
      "\n",
      "üìä IMBALANCE LEVEL: 9.4% (Moderate imbalance - 40K total)\n",
      "------------------------------------------------------------\n",
      "   üìè Dataset: 21,886 real, 18,114 fake (45.3% minority)\n",
      "\n",
      "   üß™ [13/24] Original unbalanced\n",
      "      ‚úÖ Fake F1: 0.9340 | Overall F1: 0.9399\n",
      "         Dataset: 21,886 real + 18,114 fake\n",
      "\n",
      "   üß™ [14/24] Undersampled majority\n",
      "      ‚úÖ Fake F1: 0.9491 | Overall F1: 0.9495\n",
      "         Dataset: 18,114 real + 18,114 fake\n",
      "\n",
      "   üß™ [15/24] Traditional oversampling\n",
      "      ‚úÖ Fake F1: 0.9540 | Overall F1: 0.9535\n",
      "         Dataset: 21,886 real + 21,886 fake\n",
      "\n",
      "   üß™ [16/24] 10% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9364 | Overall F1: 0.9415\n",
      "         Dataset: 21,886 real + 18,491 fake\n",
      "\n",
      "   üß™ [17/24] 50% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9430 | Overall F1: 0.9453\n",
      "         Dataset: 21,886 real + 20,000 fake\n",
      "\n",
      "   üß™ [18/24] 100% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9496 | Overall F1: 0.9493\n",
      "         Dataset: 21,886 real + 21,886 fake\n",
      "\n",
      "üìä IMBALANCE LEVEL: 25.1% (Severe imbalance - 15K total)\n",
      "------------------------------------------------------------\n",
      "   üìè Dataset: 9,386 real, 5,614 fake (37.4% minority)\n",
      "\n",
      "   üß™ [19/24] Original unbalanced\n",
      "      ‚úÖ Fake F1: 0.8906 | Overall F1: 0.9177\n",
      "         Dataset: 9,386 real + 5,614 fake\n",
      "\n",
      "   üß™ [20/24] Undersampled majority\n",
      "      ‚úÖ Fake F1: 0.9327 | Overall F1: 0.9328\n",
      "         Dataset: 5,614 real + 5,614 fake\n",
      "\n",
      "   üß™ [21/24] Traditional oversampling\n",
      "      ‚úÖ Fake F1: 0.9491 | Overall F1: 0.9480\n",
      "         Dataset: 9,386 real + 9,386 fake\n",
      "\n",
      "   üß™ [22/24] 10% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.8911 | Overall F1: 0.9149\n",
      "         Dataset: 9,386 real + 5,991 fake\n",
      "\n",
      "   üß™ [23/24] 50% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9117 | Overall F1: 0.9213\n",
      "         Dataset: 9,386 real + 7,500 fake\n",
      "\n",
      "   üß™ [24/24] 100% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9356 | Overall F1: 0.9350\n",
      "         Dataset: 9,386 real + 9,386 fake\n",
      "\n",
      "üéâ Analysis Complete! Successfully ran 24 out of 24 experiments\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive imbalance severity analysis\n",
    "\n",
    "print(\"üöÄ Starting Comprehensive Imbalance Severity Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_results = []\n",
    "experiment_count = 0\n",
    "total_experiments = len(IMBALANCE_LEVELS) * len(SAMPLING_STRATEGIES)\n",
    "\n",
    "for imbalance_level, imbalance_config in IMBALANCE_LEVELS.items():\n",
    "    \n",
    "    print(f\"\\nüìä IMBALANCE LEVEL: {imbalance_level} ({imbalance_config['description']})\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Create base datasets for this imbalance level\n",
    "    level_real, level_fake = create_imbalanced_datasets(real_tweets, fake_tweets, imbalance_config)\n",
    "    \n",
    "    minority_pct = len(level_fake) / (len(level_real) + len(level_fake)) * 100\n",
    "    print(f\"   üìè Dataset: {len(level_real):,} real, {len(level_fake):,} fake ({minority_pct:.1f}% minority)\")\n",
    "    \n",
    "    # Test each sampling strategy at this imbalance level\n",
    "    for strategy_name, strategy_description in SAMPLING_STRATEGIES.items():\n",
    "        \n",
    "        experiment_count += 1\n",
    "        print(f\"\\n   üß™ [{experiment_count:2d}/{total_experiments}] {strategy_description}\")\n",
    "        \n",
    "        try:\n",
    "            # Apply sampling strategy\n",
    "            strategy_real, strategy_fake = apply_sampling_strategy(\n",
    "                level_real, level_fake, strategy_name, synthetic_tweets\n",
    "            )\n",
    "            \n",
    "            # Run experiment\n",
    "            experiment_name = f\"{imbalance_level}_{strategy_name}\"\n",
    "            result = run_imbalance_experiment(\n",
    "                real_data=strategy_real,\n",
    "                fake_data=strategy_fake,\n",
    "                experiment_name=experiment_name\n",
    "            )\n",
    "            \n",
    "            # Add metadata\n",
    "            result['imbalance_level'] = imbalance_level\n",
    "            result['sampling_strategy'] = strategy_name\n",
    "            result['strategy_description'] = strategy_description\n",
    "            \n",
    "            all_results.append(result)\n",
    "            \n",
    "            print(f\"      ‚úÖ Fake F1: {result['fake_f1']:.4f} | Overall F1: {result['overall_f1']:.4f}\")\n",
    "            print(f\"         Dataset: {result['real_count']:,} real + {result['fake_count']:,} fake\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Error: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nüéâ Analysis Complete! Successfully ran {len(all_results)} out of {total_experiments} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• TESTING EXTREME IMBALANCE LEVEL\n",
      "==================================================\n",
      "Testing if stylistic synthetic data performs best under extreme imbalance...\n",
      "\\nüìä EXTREME IMBALANCE LEVEL:\n",
      "   50.2%: 7,456 tweets (24.7% minority, 3,772 gap) - Extreme imbalance - 7.5K total\n",
      "\\nüìè Extreme Dataset: 5,614 real, 1,842 fake (24.7% minority)\n",
      "\\nüß™ EXTREME IMBALANCE EXPERIMENTS:\n",
      "----------------------------------------\n",
      "\\n   üî¨ Original unbalanced\n",
      "      ‚úÖ Fake F1: 0.8369 | Overall F1: 0.9186\n",
      "         Dataset: 5,614 real + 1,842 fake\n",
      "\\n   üî¨ Undersampled majority\n",
      "      ‚úÖ Fake F1: 0.9073 | Overall F1: 0.9103\n",
      "         Dataset: 1,842 real + 1,842 fake\n",
      "\\n   üî¨ Traditional oversampling\n",
      "      ‚úÖ Fake F1: 0.9594 | Overall F1: 0.9586\n",
      "         Dataset: 5,614 real + 5,614 fake\n",
      "\\n   üî¨ 10% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.8523 | Overall F1: 0.9164\n",
      "         Dataset: 5,614 real + 2,219 fake\n",
      "\\n   üî¨ 50% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9109 | Overall F1: 0.9288\n",
      "         Dataset: 5,614 real + 3,728 fake\n",
      "\\n   üî¨ 100% Stylistic synthetic\n",
      "      ‚úÖ Fake F1: 0.9432 | Overall F1: 0.9430\n",
      "         Dataset: 5,614 real + 5,614 fake\n",
      "\\nüèÜ EXTREME IMBALANCE RESULTS RANKING:\n",
      "   1. Traditional oversampling: 0.9594 F1\n",
      "   2. 100% Stylistic synthetic: 0.9432 F1\n",
      "   3. 50% Stylistic synthetic: 0.9109 F1\n",
      "   4. Undersampled majority: 0.9073 F1\n",
      "   5. 10% Stylistic synthetic: 0.8523 F1\n",
      "   6. Original unbalanced: 0.8369 F1\n",
      "\\nüéØ EXTREME IMBALANCE WINNER: Traditional oversampling\n",
      "   Performance: 0.9594 F1\n",
      "   üìä Traditional methods still lead under extreme conditions\n",
      "\\nüìà STYLISTIC vs TRADITIONAL AT EXTREME LEVEL:\n",
      "   Stylistic 100%: 0.9432\n",
      "   Traditional: 0.9594\n",
      "   Difference: -0.0163 (-1.70%)\n",
      "   üìâ Traditional still ahead by 0.0163 F1\n",
      "\\nüìä Adding 6 extreme imbalance results to main analysis...\n",
      "‚úÖ Results updated with extreme imbalance experiments\n",
      "\\nüî• Extreme imbalance analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Test extreme imbalance level to find stylistic data tipping point\n",
    "\n",
    "print(\"üî• TESTING EXTREME IMBALANCE LEVEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing if stylistic synthetic data performs best under extreme imbalance...\")\n",
    "\n",
    "# Define extreme imbalance level (maintaining 3,772 gap)\n",
    "EXTREME_LEVEL = {\n",
    "    \"50.2%\": {\"real\": 5614, \"fake\": 1842, \"description\": \"Extreme imbalance - 7.5K total\"}\n",
    "}\n",
    "\n",
    "print(f\"\\\\nüìä EXTREME IMBALANCE LEVEL:\")\n",
    "for level, config in EXTREME_LEVEL.items():\n",
    "    minority_pct = config['fake'] / (config['real'] + config['fake']) * 100\n",
    "    total = config['real'] + config['fake']\n",
    "    gap = config['real'] - config['fake']\n",
    "    print(f\"   {level}: {total:,} tweets ({minority_pct:.1f}% minority, {gap:,} gap) - {config['description']}\")\n",
    "\n",
    "# Create extreme imbalance datasets\n",
    "extreme_real, extreme_fake = create_imbalanced_datasets(real_tweets, fake_tweets, EXTREME_LEVEL[\"50.2%\"])\n",
    "\n",
    "print(f\"\\\\nüìè Extreme Dataset: {len(extreme_real):,} real, {len(extreme_fake):,} fake ({len(extreme_fake)/(len(extreme_real) + len(extreme_fake))*100:.1f}% minority)\")\n",
    "\n",
    "# Test all strategies at extreme imbalance\n",
    "extreme_results = []\n",
    "\n",
    "print(f\"\\\\nüß™ EXTREME IMBALANCE EXPERIMENTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for strategy_name, strategy_description in SAMPLING_STRATEGIES.items():\n",
    "    \n",
    "    print(f\"\\\\n   üî¨ {strategy_description}\")\n",
    "    \n",
    "    try:\n",
    "        # Apply sampling strategy\n",
    "        strategy_real, strategy_fake = apply_sampling_strategy(\n",
    "            extreme_real, extreme_fake, strategy_name, synthetic_tweets\n",
    "        )\n",
    "        \n",
    "        # Run experiment\n",
    "        experiment_name = f\"50.2%_{strategy_name}\"\n",
    "        result = run_imbalance_experiment(\n",
    "            real_data=strategy_real,\n",
    "            fake_data=strategy_fake,\n",
    "            experiment_name=experiment_name\n",
    "        )\n",
    "        \n",
    "        # Add metadata\n",
    "        result['imbalance_level'] = \"50.2%\"\n",
    "        result['sampling_strategy'] = strategy_name\n",
    "        result['strategy_description'] = strategy_description\n",
    "        \n",
    "        extreme_results.append(result)\n",
    "        \n",
    "        print(f\"      ‚úÖ Fake F1: {result['fake_f1']:.4f} | Overall F1: {result['overall_f1']:.4f}\")\n",
    "        print(f\"         Dataset: {result['real_count']:,} real + {result['fake_count']:,} fake\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Analyze extreme imbalance results\n",
    "print(f\"\\\\nüèÜ EXTREME IMBALANCE RESULTS RANKING:\")\n",
    "extreme_results_sorted = sorted(extreme_results, key=lambda x: x['fake_f1'], reverse=True)\n",
    "\n",
    "for i, result in enumerate(extreme_results_sorted, 1):\n",
    "    print(f\"   {i}. {result['strategy_description']}: {result['fake_f1']:.4f} F1\")\n",
    "\n",
    "# Check if stylistic wins at extreme imbalance\n",
    "best_extreme = extreme_results_sorted[0]\n",
    "print(f\"\\\\nüéØ EXTREME IMBALANCE WINNER: {best_extreme['strategy_description']}\")\n",
    "print(f\"   Performance: {best_extreme['fake_f1']:.4f} F1\")\n",
    "\n",
    "if 'stylistic' in best_extreme['sampling_strategy']:\n",
    "    print(\"   üéâ STYLISTIC SYNTHETIC DATA WINS UNDER EXTREME IMBALANCE!\")\n",
    "else:\n",
    "    print(\"   üìä Traditional methods still lead under extreme conditions\")\n",
    "    \n",
    "# Compare stylistic 100% vs traditional at extreme level\n",
    "stylistic_extreme = next((r for r in extreme_results if r['sampling_strategy'] == 'stylistic_100'), None)\n",
    "traditional_extreme = next((r for r in extreme_results if r['sampling_strategy'] == 'random_oversampling'), None)\n",
    "\n",
    "if stylistic_extreme and traditional_extreme:\n",
    "    improvement = stylistic_extreme['fake_f1'] - traditional_extreme['fake_f1']\n",
    "    print(f\"\\\\nüìà STYLISTIC vs TRADITIONAL AT EXTREME LEVEL:\")\n",
    "    print(f\"   Stylistic 100%: {stylistic_extreme['fake_f1']:.4f}\")\n",
    "    print(f\"   Traditional: {traditional_extreme['fake_f1']:.4f}\")\n",
    "    print(f\"   Difference: {improvement:+.4f} ({improvement/traditional_extreme['fake_f1']*100:+.2f}%)\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(\"   üöÄ BREAKTHROUGH: Stylistic synthetic outperforms traditional under extreme imbalance!\")\n",
    "    else:\n",
    "        print(f\"   üìâ Traditional still ahead by {abs(improvement):.4f} F1\")\n",
    "\n",
    "# Add to main results if successful\n",
    "if extreme_results:\n",
    "    print(f\"\\\\nüìä Adding {len(extreme_results)} extreme imbalance results to main analysis...\")\n",
    "    all_results.extend(extreme_results)\n",
    "    results_df_extended = pd.DataFrame(all_results)\n",
    "    print(\"‚úÖ Results updated with extreme imbalance experiments\")\n",
    "else:\n",
    "    print(\"‚ùå No extreme imbalance results to add\")\n",
    "\n",
    "print(\"\\\\nüî• Extreme imbalance analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CORRECTED METHODOLOGY: AVOIDING DATA LEAKAGE\n",
      "============================================================\n",
      "Testing traditional oversampling vs stylistic synthetic with proper train/test splitting\n",
      "Issue: Previous experiments oversample BEFORE splitting, causing data leakage\n",
      "Fix: Split FIRST, then oversample only the TRAINING set\n",
      "\\nüß™ TESTING CORRECTED METHODOLOGY ON KEY IMBALANCE LEVELS\n",
      "------------------------------------------------------------\n",
      "\\nüìä LEVEL 9.4%: Moderate imbalance\n",
      "----------------------------------------\n",
      "\\n   üìä Level 9.4%\n",
      "      Original train set: 17,509 real, 14,491 fake\n",
      "      Test set: 8,000 tweets (untouched)\n",
      "      Traditional training: 17,509 real + 17,509 fake\n",
      "      Traditional F1: 0.9353 (fake), 0.9411 (overall)\n",
      "      Stylistic training: 17,509 real + 17,509 fake (3,018 synthetic)\n",
      "      Stylistic F1: 0.9338 (fake), 0.9398 (overall)\n",
      "      üîÑ Stylistic vs Traditional: -0.0015 F1 difference\n",
      "      üìä Traditional still ahead by 0.0015\n",
      "\\nüìä LEVEL 25.1%: Severe imbalance\n",
      "----------------------------------------\n",
      "\\n   üìä Level 25.1%\n",
      "      Original train set: 7,509 real, 4,491 fake\n",
      "      Test set: 3,000 tweets (untouched)\n",
      "      Traditional training: 7,509 real + 7,509 fake\n",
      "      Traditional F1: 0.8900 (fake), 0.9166 (overall)\n",
      "      Stylistic training: 7,509 real + 7,509 fake (3,018 synthetic)\n",
      "      Stylistic F1: 0.8843 (fake), 0.9131 (overall)\n",
      "      üîÑ Stylistic vs Traditional: -0.0057 F1 difference\n",
      "      üìä Traditional still ahead by 0.0057\n",
      "\\nüìä LEVEL 50.2%: Extreme imbalance\n",
      "----------------------------------------\n",
      "\\n   üìä Level 50.2%\n",
      "      Original train set: 4,491 real, 1,473 fake\n",
      "      Test set: 1,492 tweets (untouched)\n",
      "      Traditional training: 4,491 real + 4,491 fake\n",
      "      Traditional F1: 0.8549 (fake), 0.9265 (overall)\n",
      "      Stylistic training: 4,491 real + 4,491 fake (3,018 synthetic)\n",
      "      Stylistic F1: 0.8242 (fake), 0.9125 (overall)\n",
      "      üîÑ Stylistic vs Traditional: -0.0307 F1 difference\n",
      "      üìä Traditional still ahead by 0.0307\n",
      "\\nüéØ CORRECTED METHODOLOGY SUMMARY\n",
      "========================================\n",
      "\\nüìà 9.4%:\n",
      "   Traditional: 0.9353 F1\n",
      "   Stylistic:   0.9338 F1\n",
      "   Difference:  -0.0015 F1\n",
      "   üìä Traditional still ahead\n",
      "\\nüìà 25.1%:\n",
      "   Traditional: 0.8900 F1\n",
      "   Stylistic:   0.8843 F1\n",
      "   Difference:  -0.0057 F1\n",
      "   üìä Traditional still ahead\n",
      "\\nüìà 50.2%:\n",
      "   Traditional: 0.8549 F1\n",
      "   Stylistic:   0.8242 F1\n",
      "   Difference:  -0.0307 F1\n",
      "   üìä Traditional still ahead\n",
      "\\nüéâ FINAL CORRECTED RESULTS:\n",
      "   Stylistic synthetic wins in 0/3 severe imbalance levels\n",
      "   Data leakage correction shows mixed results for stylistic approach\n",
      "\\nüìä Corrected methodology confirms the need for careful experimental design\n",
      "\\n‚úÖ Corrected methodology analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Corrected methodology: Split BEFORE oversampling to avoid data leakage\n",
    "\n",
    "print(\"üîç CORRECTED METHODOLOGY: AVOIDING DATA LEAKAGE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing traditional oversampling vs stylistic synthetic with proper train/test splitting\")\n",
    "print(\"Issue: Previous experiments oversample BEFORE splitting, causing data leakage\")\n",
    "print(\"Fix: Split FIRST, then oversample only the TRAINING set\")\n",
    "\n",
    "def run_corrected_experiment(real_data, fake_data, experiment_name, synthetic_tweets, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Run experiment with corrected methodology: split first, then oversample training set only\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Prepare original data and split FIRST\n",
    "    texts = real_data + fake_data\n",
    "    labels = [0] * len(real_data) + [1] * len(fake_data)\n",
    "    \n",
    "    # Step 2: Train/test split on original data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Step 3: Separate training data by class\n",
    "    train_real = [text for text, label in zip(X_train, y_train) if label == 0]\n",
    "    train_fake = [text for text, label in zip(X_train, y_train) if label == 1]\n",
    "    \n",
    "    print(f\"\\\\n   üìä {experiment_name}\")\n",
    "    print(f\"      Original train set: {len(train_real):,} real, {len(train_fake):,} fake\")\n",
    "    print(f\"      Test set: {len(X_test):,} tweets (untouched)\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Traditional oversampling (applied only to training set)\n",
    "    train_imbalance = len(train_real) - len(train_fake)\n",
    "    if train_imbalance > 0:\n",
    "        # Oversample training fake tweets only\n",
    "        train_fake_oversampled = train_fake + list(resample(\n",
    "            train_fake, n_samples=train_imbalance, random_state=random_state\n",
    "        ))\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_train_traditional = train_real + train_fake_oversampled\n",
    "        y_train_traditional = [0] * len(train_real) + [1] * len(train_fake_oversampled)\n",
    "        \n",
    "        print(f\"      Traditional training: {len(train_real):,} real + {len(train_fake_oversampled):,} fake\")\n",
    "        \n",
    "        # Train and test traditional model\n",
    "        vectorizer_trad = CountVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "        X_train_trad_vec = vectorizer_trad.fit_transform(X_train_traditional)\n",
    "        X_test_trad_vec = vectorizer_trad.transform(X_test)\n",
    "        \n",
    "        classifier_trad = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
    "        classifier_trad.fit(X_train_trad_vec, y_train_traditional)\n",
    "        y_pred_trad = classifier_trad.predict(X_test_trad_vec)\n",
    "        \n",
    "        trad_fake_f1 = f1_score(y_test, y_pred_trad, pos_label=1)\n",
    "        trad_overall_f1 = f1_score(y_test, y_pred_trad, average='weighted')\n",
    "        \n",
    "        results['traditional'] = {\n",
    "            'fake_f1': trad_fake_f1,\n",
    "            'overall_f1': trad_overall_f1,\n",
    "            'train_size': len(X_train_traditional)\n",
    "        }\n",
    "        \n",
    "        print(f\"      Traditional F1: {trad_fake_f1:.4f} (fake), {trad_overall_f1:.4f} (overall)\")\n",
    "    \n",
    "    # Stylistic synthetic (applied to training set)\n",
    "    # Use portion of synthetic data proportional to the imbalance\n",
    "    synthetic_needed = min(train_imbalance, len(synthetic_tweets)) if train_imbalance > 0 else 0\n",
    "    if synthetic_needed > 0:\n",
    "        train_synthetic_sample = resample(synthetic_tweets, n_samples=synthetic_needed, random_state=random_state)\n",
    "        \n",
    "        # Prepare training data with synthetic\n",
    "        train_fake_synthetic = train_fake + train_synthetic_sample\n",
    "        X_train_synthetic = train_real + train_fake_synthetic  \n",
    "        y_train_synthetic = [0] * len(train_real) + [1] * len(train_fake_synthetic)\n",
    "        \n",
    "        print(f\"      Stylistic training: {len(train_real):,} real + {len(train_fake_synthetic):,} fake ({synthetic_needed:,} synthetic)\")\n",
    "        \n",
    "        # Train and test stylistic model\n",
    "        vectorizer_syn = CountVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "        X_train_syn_vec = vectorizer_syn.fit_transform(X_train_synthetic)\n",
    "        X_test_syn_vec = vectorizer_syn.transform(X_test)\n",
    "        \n",
    "        classifier_syn = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
    "        classifier_syn.fit(X_train_syn_vec, y_train_synthetic)\n",
    "        y_pred_syn = classifier_syn.predict(X_test_syn_vec)\n",
    "        \n",
    "        syn_fake_f1 = f1_score(y_test, y_pred_syn, pos_label=1)\n",
    "        syn_overall_f1 = f1_score(y_test, y_pred_syn, average='weighted')\n",
    "        \n",
    "        results['stylistic'] = {\n",
    "            'fake_f1': syn_fake_f1,\n",
    "            'overall_f1': syn_overall_f1,\n",
    "            'train_size': len(X_train_synthetic)\n",
    "        }\n",
    "        \n",
    "        print(f\"      Stylistic F1: {syn_fake_f1:.4f} (fake), {syn_overall_f1:.4f} (overall)\")\n",
    "        \n",
    "        # Compare results\n",
    "        if 'traditional' in results and 'stylistic' in results:\n",
    "            improvement = syn_fake_f1 - trad_fake_f1\n",
    "            print(f\"      üîÑ Stylistic vs Traditional: {improvement:+.4f} F1 difference\")\n",
    "            if improvement > 0:\n",
    "                print(f\"      üéâ STYLISTIC WINS with corrected methodology!\")\n",
    "            else:\n",
    "                print(f\"      üìä Traditional still ahead by {abs(improvement):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test corrected methodology on the three levels where traditional oversampling won\n",
    "\n",
    "print(\"\\\\nüß™ TESTING CORRECTED METHODOLOGY ON KEY IMBALANCE LEVELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Test levels where we want to verify results\n",
    "test_levels = {\n",
    "    \"9.4%\": {\"real\": 21886, \"fake\": 18114, \"description\": \"Moderate imbalance\"},\n",
    "    \"25.1%\": {\"real\": 9386, \"fake\": 5614, \"description\": \"Severe imbalance\"}, \n",
    "    \"50.2%\": {\"real\": 5614, \"fake\": 1842, \"description\": \"Extreme imbalance\"}\n",
    "}\n",
    "\n",
    "corrected_results = []\n",
    "\n",
    "for level_name, level_config in test_levels.items():\n",
    "    print(f\"\\\\nüìä LEVEL {level_name}: {level_config['description']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create datasets for this level\n",
    "    level_real, level_fake = create_imbalanced_datasets(real_tweets, fake_tweets, level_config)\n",
    "    \n",
    "    # Run corrected experiment  \n",
    "    results = run_corrected_experiment(\n",
    "        real_data=level_real,\n",
    "        fake_data=level_fake, \n",
    "        experiment_name=f\"Level {level_name}\",\n",
    "        synthetic_tweets=synthetic_tweets\n",
    "    )\n",
    "    \n",
    "    # Store results with metadata\n",
    "    for method, metrics in results.items():\n",
    "        corrected_results.append({\n",
    "            'imbalance_level': level_name,\n",
    "            'method': method,\n",
    "            'fake_f1': metrics['fake_f1'],\n",
    "            'overall_f1': metrics['overall_f1'],\n",
    "            'train_size': metrics['train_size'],\n",
    "            'methodology': 'corrected_split_first'\n",
    "        })\n",
    "\n",
    "print(\"\\\\nüéØ CORRECTED METHODOLOGY SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Analyze corrected results\n",
    "corrected_df = pd.DataFrame(corrected_results)\n",
    "\n",
    "for level in test_levels.keys():\n",
    "    level_results = corrected_df[corrected_df['imbalance_level'] == level]\n",
    "    \n",
    "    if len(level_results) >= 2:\n",
    "        traditional = level_results[level_results['method'] == 'traditional'].iloc[0]\n",
    "        stylistic = level_results[level_results['method'] == 'stylistic'].iloc[0]\n",
    "        \n",
    "        improvement = stylistic['fake_f1'] - traditional['fake_f1']\n",
    "        print(f\"\\\\nüìà {level}:\")\n",
    "        print(f\"   Traditional: {traditional['fake_f1']:.4f} F1\")  \n",
    "        print(f\"   Stylistic:   {stylistic['fake_f1']:.4f} F1\")\n",
    "        print(f\"   Difference:  {improvement:+.4f} F1\")\n",
    "        \n",
    "        if improvement > 0:\n",
    "            print(f\"   üèÜ Stylistic WINS with corrected methodology!\")\n",
    "        else:\n",
    "            print(f\"   üìä Traditional still ahead\")\n",
    "\n",
    "# Overall conclusion\n",
    "stylistic_wins = sum(1 for level in test_levels.keys() \n",
    "                    if len(corrected_df[corrected_df['imbalance_level'] == level]) >= 2 \n",
    "                    and corrected_df[(corrected_df['imbalance_level'] == level) & \n",
    "                                   (corrected_df['method'] == 'stylistic')]['fake_f1'].iloc[0] > \n",
    "                       corrected_df[(corrected_df['imbalance_level'] == level) & \n",
    "                                   (corrected_df['method'] == 'traditional')]['fake_f1'].iloc[0])\n",
    "\n",
    "print(f\"\\\\nüéâ FINAL CORRECTED RESULTS:\")\n",
    "print(f\"   Stylistic synthetic wins in {stylistic_wins}/{len(test_levels)} severe imbalance levels\")\n",
    "print(f\"   Data leakage correction {'VALIDATES' if stylistic_wins > len(test_levels)//2 else 'shows mixed results for'} stylistic approach\")\n",
    "\n",
    "if stylistic_wins > len(test_levels)//2:\n",
    "    print(\"\\\\nüöÄ BREAKTHROUGH: Corrected methodology shows stylistic synthetic data superiority!\")\n",
    "    print(\"   Previous results were contaminated by data leakage in traditional oversampling\")\n",
    "else:\n",
    "    print(\"\\\\nüìä Corrected methodology confirms the need for careful experimental design\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Corrected methodology analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä COMPREHENSIVE RESULTS ANALYSIS\n",
      "==================================================\n",
      "\n",
      "üèÜ TOP PERFORMING METHODS (by Fake F1 Score):\n",
      "   0.9733 | 2.8% | Undersampled majority\n",
      "   0.9714 | 2.8% | Traditional oversampling\n",
      "   0.9708 | 2.8% | 100% Stylistic synthetic\n",
      "   0.9696 | 2.8% | 50% Stylistic synthetic\n",
      "   0.9685 | 2.8% | 10% Stylistic synthetic\n",
      "   0.9682 | 2.8% | Original unbalanced\n",
      "\n",
      "üìà BEST METHOD PER IMBALANCE LEVEL:\n",
      "   2.8%: Undersampled majority (F1: 0.9733)\n",
      "   5.6%: Undersampled majority (F1: 0.9603)\n",
      "   9.4%: Traditional oversampling (F1: 0.9540)\n",
      "   25.1%: Traditional oversampling (F1: 0.9491)\n",
      "\n",
      "üé® STYLISTIC SYNTHETIC PERFORMANCE:\n",
      "   Average F1: 0.9366\n",
      "   Best: 100% Stylistic synthetic at 2.8% (F1: 0.9708)\n",
      "\n",
      "üîÑ STYLISTIC vs TRADITIONAL OVERSAMPLING:\n",
      "   2.8%: Stylistic -0.0006 vs Traditional (0.9708 vs 0.9714)\n",
      "   5.6%: Stylistic -0.0024 vs Traditional (0.9570 vs 0.9594)\n",
      "   9.4%: Stylistic -0.0044 vs Traditional (0.9496 vs 0.9540)\n",
      "   25.1%: Stylistic -0.0135 vs Traditional (0.9356 vs 0.9491)\n",
      "\n",
      "üìã Results DataFrame Shape: (30, 13)\n",
      "\n",
      "Ready for visualization and detailed analysis!\n"
     ]
    }
   ],
   "source": [
    "# Analyze results and create comprehensive comparison\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"üìä COMPREHENSIVE RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Overall best performing methods\n",
    "print(\"\\nüèÜ TOP PERFORMING METHODS (by Fake F1 Score):\")\n",
    "top_results = results_df.nlargest(6, 'fake_f1')\n",
    "for idx, row in top_results.iterrows():\n",
    "    print(f\"   {row['fake_f1']:.4f} | {row['imbalance_level']} | {row['strategy_description']}\")\n",
    "\n",
    "# Performance by imbalance level\n",
    "print(\"\\nüìà BEST METHOD PER IMBALANCE LEVEL:\")\n",
    "for level in IMBALANCE_LEVELS.keys():\n",
    "    level_results = results_df[results_df['imbalance_level'] == level]\n",
    "    best = level_results.loc[level_results['fake_f1'].idxmax()]\n",
    "    print(f\"   {level}: {best['strategy_description']} (F1: {best['fake_f1']:.4f})\")\n",
    "\n",
    "# Stylistic synthetic performance analysis\n",
    "print(\"\\nüé® STYLISTIC SYNTHETIC PERFORMANCE:\")\n",
    "stylistic_results = results_df[results_df['sampling_strategy'].str.contains('stylistic')]\n",
    "if not stylistic_results.empty:\n",
    "    avg_performance = stylistic_results['fake_f1'].mean()\n",
    "    best_stylistic = stylistic_results.loc[stylistic_results['fake_f1'].idxmax()]\n",
    "    print(f\"   Average F1: {avg_performance:.4f}\")\n",
    "    print(f\"   Best: {best_stylistic['strategy_description']} at {best_stylistic['imbalance_level']} (F1: {best_stylistic['fake_f1']:.4f})\")\n",
    "\n",
    "# Traditional oversampling comparison\n",
    "print(\"\\nüîÑ STYLISTIC vs TRADITIONAL OVERSAMPLING:\")\n",
    "for level in IMBALANCE_LEVELS.keys():\n",
    "    level_results = results_df[results_df['imbalance_level'] == level]\n",
    "    \n",
    "    traditional = level_results[level_results['sampling_strategy'] == 'random_oversampling']\n",
    "    stylistic_100 = level_results[level_results['sampling_strategy'] == 'stylistic_100']\n",
    "    \n",
    "    if not traditional.empty and not stylistic_100.empty:\n",
    "        trad_f1 = traditional.iloc[0]['fake_f1']\n",
    "        styl_f1 = stylistic_100.iloc[0]['fake_f1']\n",
    "        improvement = styl_f1 - trad_f1\n",
    "        print(f\"   {level}: Stylistic {improvement:+.4f} vs Traditional ({styl_f1:.4f} vs {trad_f1:.4f})\")\n",
    "\n",
    "print(f\"\\nüìã Results DataFrame Shape: {results_df.shape}\")\n",
    "print(\"\\nReady for visualization and detailed analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVING CLASSIFICATION MODELS\n",
      "========================================\n",
      "üìÅ Created directory: saved_models\n",
      "\\nüéØ SAVING MODELS FOR KEY EXPERIMENTS\n",
      "--------------------------------------------------\n",
      "\\n1. BASELINE LEVEL (2.8% imbalance):\n",
      "\\nüî¨ Baseline 2.8%\n",
      "   Train: 55,188 real, 52,170 fake | Test: 26,840\n",
      "   Traditional F1: 0.9683\n",
      "‚úÖ Saved: Baseline_2.8pct_traditional_20250818_191649\n",
      "   Model: saved_models/Baseline_2.8pct_traditional_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Baseline_2.8pct_traditional_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Baseline_2.8pct_traditional_20250818_191649_metadata.json\n",
      "   Stylistic F1: 0.9683\n",
      "‚úÖ Saved: Baseline_2.8pct_stylistic_20250818_191649\n",
      "   Model: saved_models/Baseline_2.8pct_stylistic_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Baseline_2.8pct_stylistic_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Baseline_2.8pct_stylistic_20250818_191649_metadata.json\n",
      "   Improvement: +0.0000 (Traditional wins)\n",
      "\\n2. SEVERE IMBALANCE (25.1%):\n",
      "\\nüî¨ Severe 25.1%\n",
      "   Train: 7,509 real, 4,491 fake | Test: 3,000\n",
      "   Traditional F1: 0.8900\n",
      "‚úÖ Saved: Severe_25.1pct_traditional_20250818_191649\n",
      "   Model: saved_models/Severe_25.1pct_traditional_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Severe_25.1pct_traditional_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Severe_25.1pct_traditional_20250818_191649_metadata.json\n",
      "   Stylistic F1: 0.8843\n",
      "‚úÖ Saved: Severe_25.1pct_stylistic_20250818_191649\n",
      "   Model: saved_models/Severe_25.1pct_stylistic_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Severe_25.1pct_stylistic_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Severe_25.1pct_stylistic_20250818_191649_metadata.json\n",
      "   Improvement: -0.0057 (Traditional wins)\n",
      "\\n3. EXTREME IMBALANCE (50.2%):\n",
      "\\nüî¨ Extreme 50.2%\n",
      "   Train: 4,491 real, 1,473 fake | Test: 1,492\n",
      "   Traditional F1: 0.8549\n",
      "‚úÖ Saved: Extreme_50.2pct_traditional_20250818_191649\n",
      "   Model: saved_models/Extreme_50.2pct_traditional_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Extreme_50.2pct_traditional_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Extreme_50.2pct_traditional_20250818_191649_metadata.json\n",
      "   Stylistic F1: 0.8242\n",
      "‚úÖ Saved: Extreme_50.2pct_stylistic_20250818_191649\n",
      "   Model: saved_models/Extreme_50.2pct_stylistic_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Extreme_50.2pct_stylistic_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Extreme_50.2pct_stylistic_20250818_191649_metadata.json\n",
      "   Improvement: -0.0307 (Traditional wins)\n",
      "\\nüìã MODEL INVENTORY\n",
      "------------------------------\n",
      "‚úÖ Model inventory saved: model_inventory_20250818_191649.csv\n",
      "üìÅ All models saved in: saved_models/\n",
      "üî¢ Total models saved: 6\n",
      "\\nüîÑ HOW TO LOAD SAVED MODELS:\n",
      "\n",
      "# Example: Load a model and make predictions\n",
      "import joblib\n",
      "import json\n",
      "\n",
      "# Load model and vectorizer\n",
      "model = joblib.load('saved_models/Baseline_2_8pct_stylistic_20250818_123456_model.joblib')\n",
      "vectorizer = joblib.load('saved_models/Baseline_2_8pct_stylistic_20250818_123456_vectorizer.joblib')\n",
      "\n",
      "# Load metadata\n",
      "with open('saved_models/Baseline_2_8pct_stylistic_20250818_123456_metadata.json', 'r') as f:\n",
      "    metadata = json.load(f)\n",
      "    \n",
      "print(f\"Model F1 Score: {metadata['fake_f1_score']}\")\n",
      "\n",
      "# Make predictions on new tweets\n",
      "new_tweets = [\"Your tweet text here\"]\n",
      "new_tweets_vectorized = vectorizer.transform(new_tweets)\n",
      "predictions = model.predict(new_tweets_vectorized)  # 0=real, 1=fake\n",
      "\n",
      "\\nüíæ Model saving complete!\n"
     ]
    }
   ],
   "source": [
    "# Save trained classification models and vectorizers\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üíæ SAVING CLASSIFICATION MODELS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create models directory\n",
    "models_dir = \"saved_models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"üìÅ Created directory: {models_dir}\")\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "def save_model_with_metadata(model, vectorizer, metadata, filename_prefix):\n",
    "    \"\"\"Save model, vectorizer, and metadata\"\"\"\n",
    "    \n",
    "    base_filename = f\"{filename_prefix}_{timestamp}\"\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(models_dir, f\"{base_filename}_model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "    \n",
    "    # Save vectorizer\n",
    "    vectorizer_path = os.path.join(models_dir, f\"{base_filename}_vectorizer.joblib\")\n",
    "    joblib.dump(vectorizer, vectorizer_path)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = os.path.join(models_dir, f\"{base_filename}_metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        import json\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved: {base_filename}\")\n",
    "    print(f\"   Model: {model_path}\")\n",
    "    print(f\"   Vectorizer: {vectorizer_path}\") \n",
    "    print(f\"   Metadata: {metadata_path}\")\n",
    "    \n",
    "    return {\n",
    "        'model_path': model_path,\n",
    "        'vectorizer_path': vectorizer_path,\n",
    "        'metadata_path': metadata_path\n",
    "    }\n",
    "\n",
    "# Enhanced experiment function that saves models\n",
    "def run_and_save_experiment(real_data, fake_data, experiment_name, synthetic_tweets=None, save_models=True):\n",
    "    \"\"\"\n",
    "    Run experiment and optionally save the best models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data and split FIRST (corrected methodology)\n",
    "    texts = real_data + fake_data\n",
    "    labels = [0] * len(real_data) + [1] * len(fake_data)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Separate training data by class\n",
    "    train_real = [text for text, label in zip(X_train, y_train) if label == 0]\n",
    "    train_fake = [text for text, label in zip(X_train, y_train) if label == 1]\n",
    "    \n",
    "    models_saved = {}\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\\\nüî¨ {experiment_name}\")\n",
    "    print(f\"   Train: {len(train_real):,} real, {len(train_fake):,} fake | Test: {len(X_test):,}\")\n",
    "    \n",
    "    # Traditional oversampling\n",
    "    train_imbalance = len(train_real) - len(train_fake)\n",
    "    if train_imbalance > 0:\n",
    "        train_fake_oversampled = train_fake + list(resample(\n",
    "            train_fake, n_samples=train_imbalance, random_state=42\n",
    "        ))\n",
    "        \n",
    "        # Prepare and train traditional model\n",
    "        X_train_trad = train_real + train_fake_oversampled\n",
    "        y_train_trad = [0] * len(train_real) + [1] * len(train_fake_oversampled)\n",
    "        \n",
    "        vectorizer_trad = CountVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "        X_train_trad_vec = vectorizer_trad.fit_transform(X_train_trad)\n",
    "        X_test_trad_vec = vectorizer_trad.transform(X_test)\n",
    "        \n",
    "        model_trad = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        model_trad.fit(X_train_trad_vec, y_train_trad)\n",
    "        y_pred_trad = model_trad.predict(X_test_trad_vec)\n",
    "        \n",
    "        trad_fake_f1 = f1_score(y_test, y_pred_trad, pos_label=1)\n",
    "        results['traditional'] = trad_fake_f1\n",
    "        \n",
    "        print(f\"   Traditional F1: {trad_fake_f1:.4f}\")\n",
    "        \n",
    "        # Save traditional model\n",
    "        if save_models:\n",
    "            metadata_trad = {\n",
    "                'experiment_name': f\"{experiment_name}_traditional\",\n",
    "                'method': 'traditional_oversampling',\n",
    "                'model_type': 'RandomForestClassifier',\n",
    "                'vectorizer_type': 'CountVectorizer',\n",
    "                'fake_f1_score': trad_fake_f1,\n",
    "                'train_size': len(X_train_trad),\n",
    "                'test_size': len(X_test),\n",
    "                'real_count': len(real_data),\n",
    "                'fake_count': len(fake_data),\n",
    "                'train_real_count': len(train_real),\n",
    "                'train_fake_original': len(train_fake),\n",
    "                'train_fake_oversampled': len(train_fake_oversampled),\n",
    "                'imbalance_gap': train_imbalance,\n",
    "                'generation_timestamp': timestamp,\n",
    "                'vectorizer_params': {\n",
    "                    'max_features': 5000,\n",
    "                    'ngram_range': (1, 2),\n",
    "                    'stop_words': 'english'\n",
    "                },\n",
    "                'model_params': {\n",
    "                    'n_estimators': 100,\n",
    "                    'random_state': 42\n",
    "                }\n",
    "            }\n",
    "\n",
    "            saved_paths = save_model_with_metadata(\n",
    "                model_trad, vectorizer_trad, metadata_trad,\n",
    "                f\"{experiment_name.replace(' ', '_').replace(':', '').replace('%', 'pct')}_traditional\"\n",
    "            )\n",
    "            models_saved['traditional'] = saved_paths\n",
    "\n",
    "    # Stylistic synthetic\n",
    "    if synthetic_tweets and train_imbalance > 0:\n",
    "        synthetic_needed = min(train_imbalance, len(synthetic_tweets))\n",
    "        train_synthetic_sample = resample(synthetic_tweets, n_samples=synthetic_needed, random_state=42)\n",
    "\n",
    "        # Prepare and train stylistic model\n",
    "        train_fake_synthetic = train_fake + train_synthetic_sample\n",
    "        X_train_syn = train_real + train_fake_synthetic\n",
    "        y_train_syn = [0] * len(train_real) + [1] * len(train_fake_synthetic)\n",
    "\n",
    "        vectorizer_syn = CountVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "        X_train_syn_vec = vectorizer_syn.fit_transform(X_train_syn)\n",
    "        X_test_syn_vec = vectorizer_syn.transform(X_test)\n",
    "\n",
    "        model_syn = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        model_syn.fit(X_train_syn_vec, y_train_syn)\n",
    "        y_pred_syn = model_syn.predict(X_test_syn_vec)\n",
    "\n",
    "        syn_fake_f1 = f1_score(y_test, y_pred_syn, pos_label=1)\n",
    "        results['stylistic'] = syn_fake_f1\n",
    "\n",
    "        print(f\"   Stylistic F1: {syn_fake_f1:.4f}\")\n",
    "\n",
    "        # Save stylistic model\n",
    "        if save_models:\n",
    "            metadata_syn = {\n",
    "                'experiment_name': f\"{experiment_name}_stylistic\",\n",
    "                'method': 'stylistic_synthetic',\n",
    "                'model_type': 'RandomForestClassifier',\n",
    "                'vectorizer_type': 'CountVectorizer',\n",
    "                'fake_f1_score': syn_fake_f1,\n",
    "                'train_size': len(X_train_syn),\n",
    "                'test_size': len(X_test),\n",
    "                'real_count': len(real_data),\n",
    "                'fake_count': len(fake_data),\n",
    "                'train_real_count': len(train_real),\n",
    "                'train_fake_original': len(train_fake),\n",
    "                'train_synthetic_added': len(train_synthetic_sample),\n",
    "                'synthetic_tweets_available': len(synthetic_tweets),\n",
    "                'synthetic_generation_cost': 0.3261,\n",
    "                'imbalance_gap': train_imbalance,\n",
    "                'generation_timestamp': timestamp,\n",
    "                'vectorizer_params': {\n",
    "                    'max_features': 5000,\n",
    "                    'ngram_range': (1, 2),\n",
    "                    'stop_words': 'english'\n",
    "                },\n",
    "                'model_params': {\n",
    "                    'n_estimators': 100,\n",
    "                    'random_state': 42\n",
    "                },\n",
    "                'synthetic_data_features': {\n",
    "                    'avg_word_count': synthetic_df['word_count'].mean() if 'synthetic_df' in globals() else None,\n",
    "                    'avg_exclamation_count': synthetic_df['exclamation_count'].mean() if 'synthetic_df' in globals() else None,\n",
    "                    'topics': synthetic_df['topic'].value_counts().to_dict() if 'synthetic_df' in globals() else None\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            saved_paths = save_model_with_metadata(\n",
    "                model_syn, vectorizer_syn, metadata_syn,\n",
    "                f\"{experiment_name.replace(' ', '_').replace(':', '').replace('%', 'pct')}_stylistic\"\n",
    "            )\n",
    "            models_saved['stylistic'] = saved_paths\n",
    "        \n",
    "        # Compare results\n",
    "        if 'traditional' in results:\n",
    "            improvement = syn_fake_f1 - results['traditional']\n",
    "            print(f\"   Improvement: {improvement:+.4f} ({'STYLISTIC WINS' if improvement > 0 else 'Traditional wins'})\")\n",
    "    \n",
    "    return results, models_saved\n",
    "\n",
    "# Save models for the best performing configurations\n",
    "print(\"\\\\nüéØ SAVING MODELS FOR KEY EXPERIMENTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Best overall performer from your previous results\n",
    "print(\"\\\\n1. BASELINE LEVEL (2.8% imbalance):\")\n",
    "baseline_real, baseline_fake = create_imbalanced_datasets(real_tweets, fake_tweets, \n",
    "                                                         {\"real\": 68985, \"fake\": 65213})\n",
    "baseline_results, baseline_models = run_and_save_experiment(\n",
    "    baseline_real, baseline_fake, \"Baseline 2.8%\", synthetic_tweets\n",
    ")\n",
    "\n",
    "# Severe imbalance where stylistic might win\n",
    "print(\"\\\\n2. SEVERE IMBALANCE (25.1%):\")\n",
    "severe_real, severe_fake = create_imbalanced_datasets(real_tweets, fake_tweets,\n",
    "                                                     {\"real\": 9386, \"fake\": 5614})\n",
    "severe_results, severe_models = run_and_save_experiment(\n",
    "    severe_real, severe_fake, \"Severe 25.1%\", synthetic_tweets\n",
    ")\n",
    "\n",
    "# Extreme imbalance\n",
    "print(\"\\\\n3. EXTREME IMBALANCE (50.2%):\")\n",
    "extreme_real, extreme_fake = create_imbalanced_datasets(real_tweets, fake_tweets,\n",
    "                                                       {\"real\": 5614, \"fake\": 1842})\n",
    "extreme_results, extreme_models = run_and_save_experiment(\n",
    "    extreme_real, extreme_fake, \"Extreme 50.2%\", synthetic_tweets\n",
    ")\n",
    "\n",
    "# Create model inventory\n",
    "print(\"\\\\nüìã MODEL INVENTORY\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "all_saved_models = []\n",
    "for exp_name, models_dict in [(\"Baseline\", baseline_models), (\"Severe\", severe_models), (\"Extreme\", extreme_models)]:\n",
    "    for method, paths in models_dict.items():\n",
    "        all_saved_models.append({\n",
    "            'experiment': exp_name,\n",
    "            'method': method,\n",
    "            'model_path': paths['model_path'],\n",
    "            'vectorizer_path': paths['vectorizer_path'],\n",
    "            'metadata_path': paths['metadata_path']\n",
    "        })\n",
    "\n",
    "model_inventory_df = pd.DataFrame(all_saved_models)\n",
    "inventory_file = f\"model_inventory_{timestamp}.csv\"\n",
    "model_inventory_df.to_csv(inventory_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Model inventory saved: {inventory_file}\")\n",
    "print(f\"üìÅ All models saved in: {models_dir}/\")\n",
    "print(f\"üî¢ Total models saved: {len(all_saved_models)}\")\n",
    "\n",
    "# Show how to load models\n",
    "print(\"\\\\nüîÑ HOW TO LOAD SAVED MODELS:\")\n",
    "print('''\n",
    "# Example: Load a model and make predictions\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Load model and vectorizer\n",
    "model = joblib.load('saved_models/Baseline_2_8pct_stylistic_20250818_123456_model.joblib')\n",
    "vectorizer = joblib.load('saved_models/Baseline_2_8pct_stylistic_20250818_123456_vectorizer.joblib')\n",
    "\n",
    "# Load metadata\n",
    "with open('saved_models/Baseline_2_8pct_stylistic_20250818_123456_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "    \n",
    "print(f\"Model F1 Score: {metadata['fake_f1_score']}\")\n",
    "\n",
    "# Make predictions on new tweets\n",
    "new_tweets = [\"Your tweet text here\"]\n",
    "new_tweets_vectorized = vectorizer.transform(new_tweets)\n",
    "predictions = model.predict(new_tweets_vectorized)  # 0=real, 1=fake\n",
    "''')\n",
    "\n",
    "print(\"\\\\nüíæ Model saving complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SAVING MISSING 9.4% IMBALANCE MODELS\n",
      "==================================================\n",
      "Adding the missing moderate imbalance level models to our saved collection...\n",
      "\n",
      "4. MODERATE IMBALANCE (9.4%):\n",
      "\\nüî¨ Moderate 9.4%\n",
      "   Train: 17,509 real, 14,491 fake | Test: 8,000\n",
      "   Traditional F1: 0.9353\n",
      "‚úÖ Saved: Moderate_9.4pct_traditional_20250818_191649\n",
      "   Model: saved_models/Moderate_9.4pct_traditional_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Moderate_9.4pct_traditional_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Moderate_9.4pct_traditional_20250818_191649_metadata.json\n",
      "   Stylistic F1: 0.9338\n",
      "‚úÖ Saved: Moderate_9.4pct_stylistic_20250818_191649\n",
      "   Model: saved_models/Moderate_9.4pct_stylistic_20250818_191649_model.joblib\n",
      "   Vectorizer: saved_models/Moderate_9.4pct_stylistic_20250818_191649_vectorizer.joblib\n",
      "   Metadata: saved_models/Moderate_9.4pct_stylistic_20250818_191649_metadata.json\n",
      "   Improvement: -0.0015 (Traditional wins)\n",
      "\n",
      "üìã UPDATING MODEL INVENTORY\n",
      "------------------------------\n",
      "‚úÖ Updated model inventory saved: model_inventory_updated_20250818_191649.csv\n",
      "üî¢ Total models now saved: 8\n",
      "\n",
      "üìä COMPLETE MODEL COLLECTION:\n",
      "   1. Baseline - traditional\n",
      "   2. Baseline - stylistic\n",
      "   3. Severe - traditional\n",
      "   4. Severe - stylistic\n",
      "   5. Extreme - traditional\n",
      "   6. Extreme - stylistic\n",
      "   7. Moderate - traditional\n",
      "   8. Moderate - stylistic\n",
      "\n",
      "‚úÖ VERIFICATION - Models per imbalance level:\n",
      "   Baseline: 2 models (‚úÖ)\n",
      "   Severe: 2 models (‚úÖ)\n",
      "   Extreme: 2 models (‚úÖ)\n",
      "   Moderate: 2 models (‚úÖ)\n",
      "\n",
      "üéâ SUCCESS: All 4 imbalance levels now have 2 models each (traditional + stylistic)\n",
      "   Total: 8 models saved\n",
      "\n",
      "üíæ Model saving update complete!\n"
     ]
    }
   ],
   "source": [
    "# Save missing 9.4% imbalance level models\n",
    "\n",
    "print(\"üîß SAVING MISSING 9.4% IMBALANCE MODELS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Adding the missing moderate imbalance level models to our saved collection...\")\n",
    "\n",
    "# Save 9.4% imbalance models\n",
    "print(\"\\n4. MODERATE IMBALANCE (9.4%):\")\n",
    "moderate_real, moderate_fake = create_imbalanced_datasets(real_tweets, fake_tweets,\n",
    "                                                         {\"real\": 21886, \"fake\": 18114})\n",
    "moderate_results, moderate_models = run_and_save_experiment(\n",
    "    moderate_real, moderate_fake, \"Moderate 9.4%\", synthetic_tweets\n",
    ")\n",
    "\n",
    "# Update model inventory with the new models\n",
    "print(\"\\nüìã UPDATING MODEL INVENTORY\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Load existing inventory\n",
    "existing_inventory = pd.read_csv(f\"model_inventory_{timestamp}.csv\")\n",
    "\n",
    "# Add new models to inventory\n",
    "new_models = []\n",
    "for method, paths in moderate_models.items():\n",
    "    new_models.append({\n",
    "        'experiment': \"Moderate\",\n",
    "        'method': method,\n",
    "        'model_path': paths['model_path'],\n",
    "        'vectorizer_path': paths['vectorizer_path'],\n",
    "        'metadata_path': paths['metadata_path']\n",
    "    })\n",
    "\n",
    "new_models_df = pd.DataFrame(new_models)\n",
    "updated_inventory = pd.concat([existing_inventory, new_models_df], ignore_index=True)\n",
    "\n",
    "# Save updated inventory\n",
    "updated_inventory_file = f\"model_inventory_updated_{timestamp}.csv\"\n",
    "updated_inventory.to_csv(updated_inventory_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Updated model inventory saved: {updated_inventory_file}\")\n",
    "print(f\"üî¢ Total models now saved: {len(updated_inventory)}\")\n",
    "\n",
    "# Show complete model collection\n",
    "print(f\"\\nüìä COMPLETE MODEL COLLECTION:\")\n",
    "for idx, row in updated_inventory.iterrows():\n",
    "    print(f\"   {idx+1}. {row['experiment']} - {row['method']}\")\n",
    "\n",
    "# Verify we have all imbalance levels - FIXED VERSION\n",
    "imbalance_levels = updated_inventory['experiment'].value_counts()\n",
    "print(f\"\\n‚úÖ VERIFICATION - Models per imbalance level:\")\n",
    "for level, count in imbalance_levels.items():\n",
    "    print(f\"   {level}: {count} models ({'‚úÖ' if count == 2 else '‚ùå'})\")\n",
    "\n",
    "# Fixed: Access .values directly (not as function)\n",
    "level_counts = list(imbalance_levels.values)\n",
    "if len(imbalance_levels) == 4 and all(count == 2 for count in level_counts):\n",
    "    print(f\"\\nüéâ SUCCESS: All 4 imbalance levels now have 2 models each (traditional + stylistic)\")\n",
    "    print(f\"   Total: {len(updated_inventory)} models saved\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Still missing some models - please check the collection\")\n",
    "    print(f\"   Found {len(imbalance_levels)} imbalance levels\")\n",
    "    print(f\"   Model counts: {level_counts}\")\n",
    "\n",
    "print(f\"\\nüíæ Model saving update complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
